{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save links and tags for future reference (SVM)\n",
    "# \"https://www.analyticsvidhya.com/blog/2021/05/top-15-questions-to-test-your-data-science-skills-on-svm/\" => [\"h3\", \"p\"]\n",
    "# \"https://alekhyo.medium.com/interview-questions-on-svm-bf13e5fbcca8\" => [{\"class\":\"hv da\"},\"p\"]\n",
    "# \"https://towardsdatascience.com/support-vector-machine-svm-719e530a725f\" => [\"ol\", \"ul\", \"p\"]\n",
    "# \"https://medium.datadriveninvestor.com/support-vector-machines-important-questions-a47224692495\" => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449cb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10280662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AboutAITechBlockchainFinanceEconomicsStartupStart Here',\n",
       " 'Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?',\n",
       " 'Ans. The performance depends on many factors',\n",
       " 'the number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your model',\n",
       " 'In general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.',\n",
       " 'Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.',\n",
       " 'So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)',\n",
       " '2. Why SVM is an example of a large margin classifier?',\n",
       " 'Why SVM is an example of a large margin classifier?',\n",
       " 'SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.',\n",
       " '3. What is the role of C in SVM?',\n",
       " 'What is the role of C in SVM?',\n",
       " 'Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.',\n",
       " '4. What is the intuition of a large margin classifier?',\n",
       " 'What is the intuition of a large margin classifier?',\n",
       " 'Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.',\n",
       " 'So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.',\n",
       " 'If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.',\n",
       " '5. What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.',\n",
       " '6. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Ans.',\n",
       " 'Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy',\n",
       " '7. What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'Ans. Only in implementation, One is much more efficient and has good optimization packages',\n",
       " '8. What is the difference between logistic regression and SVM',\n",
       " 'What is the difference between logistic regression and SVM',\n",
       " 'Ans. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.',\n",
       " 'On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.',\n",
       " 'Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.',\n",
       " 'SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.',\n",
       " 'Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.',\n",
       " 'Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.',\n",
       " '9. Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.',\n",
       " 'For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.',\n",
       " 'For a higher gamma, the model will capture the shape of the dataset well.',\n",
       " '10. What is generalization error in terms of the SVM?',\n",
       " 'What is generalization error in terms of the SVM?',\n",
       " 'Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.',\n",
       " 'External Links',\n",
       " 'https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance',\n",
       " 'https://www.quora.com/Why-do-we-call-an-SVM-a-large-margin-classifier#',\n",
       " 'https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel',\n",
       " 'https://www.quora.com/What-is-the-intuition-behind-margin-in-SVM',\n",
       " 'https://www.quora.com/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression',\n",
       " 'https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/',\n",
       " 'https://stats.stackexchange.com/questions/43996/kernel-logistic-regression-vs-svm',\n",
       " 'I will add more in Future…….',\n",
       " 'empowerment through data, knowledge, and expertise.',\n",
       " '68 ',\n",
       " 'Machine LearningData ScienceArtificial IntelligenceSvmAlgorithms',\n",
       " '68\\xa0claps',\n",
       " '68 ',\n",
       " 'empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com',\n",
       " 'Written by',\n",
       " 'Artificial Intelligence Research & Development Engineer',\n",
       " 'empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = []\n",
    "url = \"https://medium.datadriveninvestor.com/support-vector-machines-important-questions-a47224692495\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([\"strong\", {\"class\":\"ho in\"}, \"ul\", \"p\"])\n",
    "for answer in answers:\n",
    "    lst1.append(answer.text)\n",
    "lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39745d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_new1 = lst1[1:44]\n",
    "# lst_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4fc641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?</td>\n",
       "      <td>Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why SVM is an example of a large margin classifier?</td>\n",
       "      <td>Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the role of C in SVM?</td>\n",
       "      <td>What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the intuition of a large margin classifier?</td>\n",
       "      <td>What is the intuition of a large margin classifier?Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is a kernel in SVM? Why do we use kernels in SVM?</td>\n",
       "      <td>What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       Questions  \\\n",
       "0  Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?   \n",
       "1                                                            Why SVM is an example of a large margin classifier?   \n",
       "2                                                                                  What is the role of C in SVM?   \n",
       "3                                                            What is the intuition of a large margin classifier?   \n",
       "4                                                         What is a kernel in SVM? Why do we use kernels in SVM?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Answer  \n",
       "0  Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.  \n",
       "3                                                                                                                                                     What is the intuition of a large margin classifier?Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^G|\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new1:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a72d04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</td>\n",
       "      <td>Can we apply the kernel trick to logistic regression? Why is it not used in practice then?Ans.Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the difference between logistic regression and SVM without a kernel?</td>\n",
       "      <td>What is the difference between logistic regression and SVM without a kernel?Ans. Only in implementation, One is much more efficient and has good optimization packages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the difference between logistic regression and SVM</td>\n",
       "      <td>What is the difference between logistic regression and SVMAns. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?</td>\n",
       "      <td>Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is generalization error in terms of the SVM?</td>\n",
       "      <td>What is generalization error in terms of the SVM?Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      Questions  \\\n",
       "5    Can we apply the kernel trick to logistic regression? Why is it not used in practice then?   \n",
       "6                  What is the difference between logistic regression and SVM without a kernel?   \n",
       "7                                    What is the difference between logistic regression and SVM   \n",
       "8        Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?   \n",
       "9                                             What is generalization error in terms of the SVM?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Answer  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Can we apply the kernel trick to logistic regression? Why is it not used in practice then?Ans.Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What is the difference between logistic regression and SVM without a kernel?Ans. Only in implementation, One is much more efficient and has good optimization packages  \n",
       "7  What is the difference between logistic regression and SVMAns. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well.  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           What is generalization error in terms of the SVM?Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('output.csv',  index=False) #encoding='utf-8',\n",
    "svm4 = pd.read_csv(\"output.csv\")\n",
    "svm4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eae856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
